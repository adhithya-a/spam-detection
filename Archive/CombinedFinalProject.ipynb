{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c699b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce6cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeaderTrain():\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.read_csv('preprocessed_spam_ham_phishing.csv')\n",
    "\n",
    "    df = df[df['label'] != 2]\n",
    " \n",
    "    \n",
    "    df['label'].value_counts()\n",
    "\n",
    "    feature_list = ['time_zone',\n",
    "    'str_content-type_texthtml',\n",
    "    'str_to_chevron',\n",
    "    'domain_match_from_return-path',\n",
    "    'missing_importance',\n",
    "    'missing_x-mailer',\n",
    "    'date_comp_date_received',\n",
    "    'str_return-path_bounce',\n",
    "    'missing_user-agent',\n",
    "    'length_from',\n",
    "    'missing_thread-index',\n",
    "    'missing_mime-version',\n",
    "    'domain_val_message-id',\n",
    "    'str_from_question',\n",
    "    'str_from_chevron',\n",
    "    'domain_match_message-id_from',\n",
    "    'missing_domainkey-signature',\n",
    "    'missing_x-mailing-list',\n",
    "    'domain_match_message-id_return-path',\n",
    "    'missing_content-disposition',\n",
    "    'missing_x-mailman-version',\n",
    "    'domain_match_to_from',\n",
    "    'missing_list-unsubscribe',\n",
    "    'domain_match_errors-to_from',\n",
    "    'span_time',\n",
    "    'domain_match_message-id_reply-to',\n",
    "    'content-length',\n",
    "    'lines',\n",
    "    'day_of_week',\n",
    "    'missing_precedence',\n",
    "    'domain_match_errors-to_message-id',\n",
    "    'missing_reply-to',\n",
    "    'domain_match_sender_from',\n",
    "    'missing_mailing-list',\n",
    "    'received_str_forged',\n",
    "    'str_precedence_list',\n",
    "    'domain_match_to_received',\n",
    "    'missing_x-spam-status',\n",
    "    'missing_content-type',\n",
    "    'content-encoding-val',\n",
    "    'domain_match_errors-to_reply-to',\n",
    "    'missing_received-spf',\n",
    "    'missing_references',\n",
    "    'domain_match_to_message-id',\n",
    "    'missing_x-original-to',\n",
    "    'label']\n",
    "\n",
    "    df = df[feature_list]\n",
    "    \n",
    "    df_Y = df['label']\n",
    "    df_X = df.drop('label', axis=1)\n",
    "    \n",
    "  \n",
    "\n",
    "    features_list = df_X.columns\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(df_X)\n",
    "    df_X = scaler.transform(df_X)\n",
    "    df_X = pd.DataFrame(df_X, columns=features_list)\n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_X,df_Y,test_size = 0.75, random_state = 42)\n",
    "    \n",
    "    \n",
    "    first_col = X_test.iloc[:,:1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "    '''\n",
    "    base_learners = [('rf', RandomForestClassifier(criterion='entropy', max_features='auto', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                    ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                    ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "                    ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "    '''\n",
    "\n",
    "    base_learners_set1 = [('rf', RandomForestClassifier(criterion='entropy', max_features='auto', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                    ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "                    ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance'))]\n",
    "\n",
    "    base_learners_set2 = [('rf', RandomForestClassifier(criterion='entropy', max_features='auto', min_samples_leaf=1, min_samples_split=3, n_estimators=100)), \n",
    "                    ('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')), \n",
    "                    ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "#     base_learners_set3 = [('rf', RandomForestClassifier(criterion='entropy', max_features='auto', min_samples_leaf=1, min_samples_split=3, n_estimators=100)),\n",
    "#                     ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "#                     ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "#     base_learners_set4 = [('mlp', MLPClassifier(max_iter=500, activation='relu', alpha=0.001, hidden_layer_sizes=(20,), learning_rate='adaptive', solver='adam')),\n",
    "#                     ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=15, n_neighbors=20, p=1, weights='distance')), \n",
    "#                     ('svm', SVC(C=10, kernel='rbf', tol=0.001))]\n",
    "\n",
    "    base_learners = []\n",
    "    base_learners.append(base_learners_set1)\n",
    "    base_learners.append(base_learners_set2)\n",
    "#     base_learners.append(base_learners_set3)\n",
    "#     base_learners.append(base_learners_set4)\n",
    "\n",
    "    for base_learner_group in base_learners:\n",
    "\n",
    "        meta_learner = LogisticRegression()\n",
    "\n",
    "        clf = StackingClassifier(estimators=base_learner_group, final_estimator=meta_learner)\n",
    "\n",
    "        # Train the stacked model on the full training data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        predictions = clf.predict(X_test)\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        print('Accuracy:', accuracy_score(y_test, predictions)*100)\n",
    "        print('F1 Score:', f1_score(y_test, predictions)*100)\n",
    "        print('Recall:', recall_score(y_test, predictions)*100)\n",
    "        print('Precision:', precision_score(y_test, predictions)*100)\n",
    "        print('ROC AUC:', roc_auc_score(y_test, predictions)*100)\n",
    "        print('Confusion Matrix:', confusion_matrix(y_test, predictions))\n",
    "        print('-----------------------------------------\\n')\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5d543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.parser import HeaderParser\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from os import listdir, mkdir, path\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import time\n",
    "from collections import Counter\n",
    "parser = HeaderParser()\n",
    "list_of_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac031e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HeaderInput(textfilename):\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "\n",
    "        columns = ['received1',\n",
    "        'received2',\n",
    "        'received3',\n",
    "        'received4',\n",
    "        'received5',\n",
    "        'received6',\n",
    "        'received7',\n",
    "        'received8',\n",
    "        'hops',\n",
    "        'subject',\n",
    "        'date',\n",
    "        'message-id',\n",
    "        'from',\n",
    "        'return-path',\n",
    "        'to',\n",
    "        'content-type',\n",
    "        'mime-version',\n",
    "        'x-mailer',\n",
    "        'content-transfer-encoding',\n",
    "        'x-mimeole',\n",
    "        'x-priority',\n",
    "        'list-id',\n",
    "        'lines',\n",
    "        'x-virus-scanned',\n",
    "        'status',\n",
    "        'content-length',\n",
    "        'precedence',\n",
    "        'delivered-to',\n",
    "        'list-unsubscribe',\n",
    "        'list-subscribe',\n",
    "        'list-post',\n",
    "        'list-help',\n",
    "        'x-msmail-priority',\n",
    "        'x-spam-status',\n",
    "        'sender',\n",
    "        'errors-to',\n",
    "        'x-beenthere',\n",
    "        'list-archive',\n",
    "        'reply-to',\n",
    "        'x-mailman-version',\n",
    "        'x-miltered',\n",
    "        'x-uuid',\n",
    "        'x-virus-status',\n",
    "        'x-spam-level',\n",
    "        'x-spam-checker-version',\n",
    "        'references',\n",
    "        'in-reply-to',\n",
    "        'user-agent',\n",
    "        'thread-index',\n",
    "        'cc',\n",
    "        'received-spf',\n",
    "        'x-original-to',\n",
    "        'content-disposition',\n",
    "        'mailing-list',\n",
    "        'x-spam-check-by',\n",
    "        'domainkey-signature',\n",
    "        'importance',\n",
    "        'x-mailing-list',\n",
    "        'label']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        def addEmailsToDict(email_list):\n",
    "            global parser, list_of_rows\n",
    "\n",
    "            # The label for phishing, set to '2'\n",
    "            label = 2\n",
    "\n",
    "\n",
    "            # Read the full email content\n",
    "            for email in email_list:\n",
    "                row_dict = {}\n",
    "\n",
    "                # Parse the email content\n",
    "                h = parser.parsestr(email)\n",
    "\n",
    "                # Parse recieved field\n",
    "                received_list = h.get_all('received')\n",
    "                hops = 0\n",
    "                if received_list is not None:\n",
    "                    hops = len(received_list)\n",
    "                    col_name_recieved = 'received'\n",
    "\n",
    "                    for inx, received_field in enumerate(received_list):\n",
    "                        col = col_name_recieved + str(inx+1)\n",
    "                        row_dict[col] = received_field\n",
    "\n",
    "\n",
    "                # Make everything lowercase to avoid issues\n",
    "                features_lower_case = [x.lower() for x in h.keys()]\n",
    "\n",
    "                # Parse everything else\n",
    "                new_row = dict(zip(features_lower_case, h.values()))\n",
    "                new_row['hops'] = hops\n",
    "\n",
    "\n",
    "                for key,value in new_row.items():\n",
    "                    if key in columns:\n",
    "                        row_dict['label'] = label\n",
    "                        row_dict[key] = value\n",
    "\n",
    "\n",
    "                list_of_rows.append(row_dict)\n",
    "\n",
    "        def main():\n",
    "            global list_of_rows\n",
    "\n",
    "            file_path = textfilename\n",
    "\n",
    "            # Read the full email content\n",
    "            emailStr = ''\n",
    "            try:\n",
    "                with open(file_path, encoding='latin_1') as emailFile:\n",
    "                    for line in emailFile:\n",
    "                        emailStr += line\n",
    "            except UnicodeDecodeError:\n",
    "                print('Unicode Error!')\n",
    "\n",
    "            email_list = emailStr.split('\\nFrom jose@monkey.org')\n",
    "            #print(email_list[1])\n",
    "\n",
    "            addEmailsToDict(email_list)\n",
    "\n",
    "            # Create the dataframe\n",
    "            df = pd.DataFrame(list_of_rows)\n",
    "\n",
    "            final_columns = []\n",
    "            for col in columns:\n",
    "                if col in df.columns:\n",
    "                    final_columns.append(col)\n",
    "\n",
    "            df = df[final_columns]\n",
    "\n",
    "            # Output the dataframe to a .csv file\n",
    "            df.to_csv('spamTest_out_2021.csv', index=False)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        FILE_NAME2021 = 'spamTest_out_2021.csv'\n",
    "\n",
    "        df = pd.read_csv(FILE_NAME2021)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "\n",
    "        initial_features_list = ['received1', 'received2', 'received3', 'received4', 'received5',\n",
    "           'received6', 'received7', 'received8', 'received9', 'received10',\n",
    "           'received11', 'received12', 'received13', 'received14',\n",
    "           'received15', 'received16', 'subject', 'date',\n",
    "           'message-id', 'from', 'return-path', 'to', 'content-type',\n",
    "           'mime-version', 'x-mailer', 'content-transfer-encoding',\n",
    "           'x-mimeole', 'x-priority', 'list-id', 'lines', 'x-virus-scanned',\n",
    "           'status', 'content-length', 'precedence', 'delivered-to',\n",
    "           'list-unsubscribe', 'list-subscribe', 'list-post', 'list-help',\n",
    "           'x-msmail-priority', 'x-spam-status', 'sender', 'errors-to',\n",
    "           'x-beenthere', 'list-archive', 'reply-to', 'x-mailman-version',\n",
    "           'x-miltered', 'x-uuid', 'x-virus-status', 'x-spam-level',\n",
    "           'x-spam-checker-version', 'references', 'in-reply-to',\n",
    "           'user-agent', 'thread-index', 'cc', 'received-spf',\n",
    "           'x-original-to', 'content-disposition', 'mailing-list',\n",
    "           'x-spam-check-by', 'domainkey-signature', 'importance',\n",
    "           'x-mailing-list']\n",
    "\n",
    "        label_name = 'label'\n",
    "\n",
    "        final_features_list = ['hops']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        missing_feature_names = []\n",
    "\n",
    "        for name in initial_features_list:\n",
    "            missing_feature_names.append('missing_'+name)\n",
    "\n",
    "        for feature, name in zip(initial_features_list, missing_feature_names):\n",
    "            if feature in df:\n",
    "                df.loc[df[feature].isnull(),name] = 1\n",
    "                df.loc[-df[feature].isnull(), name] = 0\n",
    "\n",
    "        final_features_list.extend(missing_feature_names)\n",
    "\n",
    "\n",
    "        df = df.replace(np.nan, '', regex=True)\n",
    "\n",
    "\n",
    "\n",
    "        df[~(df['content-transfer-encoding'].str.contains('(?i)8bit')) & ~(df['content-transfer-encoding'].str.contains('(?i)7bit'))]['label'].value_counts()\n",
    "\n",
    "\n",
    "        import re\n",
    "\n",
    "        def content_encoding_val(row):\n",
    "          if 'content-transfer-encoding' in row:\n",
    "              val = row['content-transfer-encoding']\n",
    "              re1 = re.compile(r'(?i)8bit')\n",
    "              re2 = re.compile(r'(?i)7bit')\n",
    "              if (not re1.search(val)) and (not re2.search(val)):\n",
    "                return 1\n",
    "              else:\n",
    "                return 0\n",
    "\n",
    "\n",
    "        df['content-encoding-val'] = df.apply(content_encoding_val, axis=1)\n",
    "\n",
    "\n",
    "        final_features_list.append('content-encoding-val')\n",
    "\n",
    "\n",
    "\n",
    "        def check_received_forged(row):\n",
    "            num_iters = row['hops']\n",
    "            col_name_base = 'reveived'\n",
    "\n",
    "            for i in range(1,num_iters+1):\n",
    "                if col_name_base + str(i) in row:\n",
    "                    curr_val = row[col_name_base + str(i)]\n",
    "                    if 'forged' in curr_val:\n",
    "                        return 1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            return 0\n",
    "\n",
    "        df['received_str_forged'] = df.apply(check_received_forged, axis=1)\n",
    "        final_features_list.append('received_str_forged')\n",
    "\n",
    "\n",
    "        df[df['received_str_forged'] == 1]['label'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "        def str_based_features_add(old_col_name, new_col_names, items_to_check):\n",
    "\n",
    "          for col_name, item_to_check in zip(new_col_names, items_to_check):\n",
    "            if item_to_check == '':\n",
    "              df.loc[(df[old_col_name].str.fullmatch(item_to_check)), col_name] = 1\n",
    "              df.loc[~(df[old_col_name].str.fullmatch(item_to_check)), col_name] = 0\n",
    "            else:\n",
    "              df.loc[(df[old_col_name].str.contains('(?i)' + item_to_check)), col_name] = 1\n",
    "              df.loc[~(df[old_col_name].str.contains('(?i)' + item_to_check)), col_name] = 0\n",
    "\n",
    "          final_features_list.extend(new_col_names)\n",
    "\n",
    "\n",
    "\n",
    "        # Content-Transfer-Encoding\n",
    "        new_col_names = ['str_content-encoding_empty']\n",
    "        items_to_check = ['']\n",
    "        feature = 'content-transfer-encoding'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # From\n",
    "        new_col_names = ['str_from_question', 'str_from_exclam', 'str_from_chevron']\n",
    "        items_to_check = ['\\?', '!', '<.+>']\n",
    "        feature = 'from'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # To\n",
    "        new_col_names = ['str_to_chevron', 'str_to_undisclosed', 'str_to_empty']\n",
    "        items_to_check = ['<.+>', 'Undisclosed Recipients', '']\n",
    "        feature = 'to'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Message-ID\n",
    "        new_col_names = ['str_message-ID_dollar']\n",
    "        items_to_check = ['\\$']\n",
    "        feature = 'message-id'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Return-Path\n",
    "        new_col_names = ['str_return-path_bounce', 'str_return-path_empty']\n",
    "        items_to_check = ['bounce', '']\n",
    "        feature = 'return-path'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Reply-To\n",
    "        new_col_names = ['str_reply-to_question']\n",
    "        items_to_check = ['\\?']\n",
    "        feature = 'reply-to'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Received-SPF\n",
    "        new_col_names = ['str_received-SPF_bad', 'str_received-SPF_softfail', \n",
    "                         'str_received-SPF_fail']\n",
    "        items_to_check = ['bad', 'softfail', 'fail']\n",
    "        feature = 'received-spf'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Content-Type\n",
    "        new_col_names = ['str_content-type_texthtml']\n",
    "        items_to_check = ['text/html']\n",
    "        feature = 'content-type'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "        # Precedence\n",
    "        new_col_names = ['str_precedence_list']\n",
    "        items_to_check = ['list']\n",
    "        feature = 'precedence'\n",
    "        str_based_features_add(feature, new_col_names, items_to_check)\n",
    "\n",
    "\n",
    "        df[df['hops'] > 5]['label'].value_counts()\n",
    "        df['hops'] = df['hops'].apply(lambda x: 0 if x <= 2 else 1 if x <= 5 else 2)\n",
    "\n",
    "\n",
    "        def count_chars(field_names, new_col_names):\n",
    "          for field_name, new_col_name in zip(field_names, new_col_names):\n",
    "            df[new_col_name] = df[field_name].str.len()\n",
    "\n",
    "          final_features_list.extend(new_col_names)\n",
    "\n",
    "\n",
    "\n",
    "        fields_to_find_lengths = ['from']\n",
    "        new_col_names_lengths = []\n",
    "\n",
    "        for val in fields_to_find_lengths:\n",
    "          new_col_names_lengths.append('length_' + val)\n",
    "\n",
    "        count_chars(fields_to_find_lengths, new_col_names_lengths)\n",
    "\n",
    "\n",
    "        df[df['length_from'] > 40]['label'].value_counts()\n",
    "\n",
    "        df['length_from'] = df['length_from'].apply(lambda x: 0 if x > 40 else 1)\n",
    "\n",
    "\n",
    "        import re\n",
    "\n",
    "        #https://stackoverflow.com/questions/42407785/regex-extract-email-from-strings\n",
    "\n",
    "        if 'to' in df:\n",
    "            df['num_recipients_to'] = df.apply(lambda x: len(re.findall(\n",
    "                r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x['to'])), axis=1)\n",
    "\n",
    "        if 'cc' in df:\n",
    "            df['num_recipients_cc'] = df.apply(lambda x: len(re.findall(\n",
    "                r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x['cc'])), axis=1)\n",
    "\n",
    "        if 'from' in df:\n",
    "\n",
    "            df['num_recipients_from'] = df.apply(lambda x: len(re.findall(\n",
    "                r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', x['from'])), axis=1)\n",
    "\n",
    "        final_features_list.append('num_recipients_to')\n",
    "        final_features_list.append('num_recipients_cc')\n",
    "        final_features_list.append('num_recipients_from')\n",
    "\n",
    "\n",
    "        df[df['num_recipients_to'] == 0]['label'].value_counts()\n",
    "\n",
    "\n",
    "        df['num_recipients_to'] = df['num_recipients_to'].apply(lambda x: 0 if x == 0 else 1 if x == 1 else 2)\n",
    "\n",
    "        if 'num_recipients_cc' in df:\n",
    "\n",
    "            df['num_recipients_cc'] = df['num_recipients_cc'].apply(lambda x: 0 if x == 0 else 1 if x == 1 else 2)\n",
    "\n",
    "        df[df['num_recipients_from'] == 0]['label'].value_counts()\n",
    "\n",
    "        df['num_recipients_from'] = df['num_recipients_from'].apply(lambda x: 0 if x == 0 else 1 if x == 1 else 2)\n",
    "\n",
    "\n",
    "\n",
    "        import re\n",
    "\n",
    "        def extract_num_replies(row):\n",
    "          references_val = row['references']\n",
    "          all = re.findall(r'<([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)>', \n",
    "                          references_val)\n",
    "          return len(all)\n",
    "\n",
    "\n",
    "        df['number_replies'] = df.apply(extract_num_replies, axis=1)\n",
    "\n",
    "        df[df['number_replies'] >= 1]['label'].value_counts()\n",
    "\n",
    "        df['number_replies'] = df['number_replies'].apply(lambda x: 0 if x >= 1 else 1)\n",
    "\n",
    "        final_features_list.append('number_replies')\n",
    "\n",
    "        from dateutil.parser import parse\n",
    "        from datetime import datetime, timedelta\n",
    "        import email.utils\n",
    "\n",
    "\n",
    "        def extract_time_zone(row):\n",
    "          time_zone = email.utils.parsedate_tz(row['date'])\n",
    "          if time_zone is None:\n",
    "            return 'NA'\n",
    "          else:\n",
    "            return int(time_zone[9] / (60*60)) % 24\n",
    "\n",
    "\n",
    "        df['time_zone'] = df.apply(extract_time_zone, axis=1)\n",
    "\n",
    "\n",
    "        df.loc[df['time_zone'].astype(str).str.contains('NA'), 'time_zone'] = df['time_zone'].value_counts().index[0]\n",
    "\n",
    "\n",
    "        # Test\n",
    "        d1 = email.utils.parsedate_tz('Mon, 9 Apr 2007 14:31:03 02300')\n",
    "        d2 = email.utils.parsedate_tz('Mon, 8 Apr 2007 14:31:03 -0100')\n",
    "\n",
    "        print((email.utils.mktime_tz(d2)) - (email.utils.mktime_tz(d1)))\n",
    "\n",
    "        df.loc[df['time_zone'] != 20, 'time_zone'] = 0\n",
    "        df.loc[df['time_zone'] == 20, 'time_zone'] = 1\n",
    "\n",
    "        final_features_list.append('time_zone')\n",
    "\n",
    "        df['x-priority'] = df['x-priority'].astype(str).str.extract('(\\\\d+)')\n",
    "        df['x-priority'] = pd.to_numeric(df['x-priority'], errors='coerce')\n",
    "        df['x-priority'] = df['x-priority'].fillna(0)\n",
    "\n",
    "\n",
    "        df['x-priority'] = df['x-priority'].apply(lambda x: 0 if x != 3 else 1)\n",
    "\n",
    "        final_features_list.append('x-priority')\n",
    "\n",
    "        if 'content-length' in df:\n",
    "\n",
    "            df['content-length'] = df['content-length'].astype(str).str.extract('(\\\\d+)')\n",
    "            df['content-length'] = pd.to_numeric(df['content-length'], errors='coerce')\n",
    "            df['content-length'] = df['content-length'].fillna(0)\n",
    "\n",
    "            df[df['content-length'] > 0]['content-length'].describe()\n",
    "\n",
    "            df['content-length'] = df['content-length'].apply(lambda x: 0 if x < 1 else 1 if x < 1274 else 2 if x < 2348 else 3 if x < 5798 else 4)\n",
    "\n",
    "        final_features_list.append('content-length')\n",
    "\n",
    "        if 'lines' in df:\n",
    "\n",
    "            df['lines'] = df['lines'].astype(str).str.extract('(\\\\d+)')\n",
    "            df['lines'] = pd.to_numeric(df['lines'], errors='coerce')\n",
    "\n",
    "            df['lines'].describe()\n",
    "\n",
    "            df['lines'] = df['lines'].fillna(0)\n",
    "            df['lines'] = df['lines'].apply(lambda x: 0 if x == 0 else 1 if x <= 30 else 2 if x <= 54 else 3 if x <= 119 else 4)\n",
    "            df['lines'].value_counts()\n",
    "\n",
    "        final_features_list.append('lines')\n",
    "\n",
    "        if 'last_reveived_date' in df:\n",
    "\n",
    "            df['last_received_date'] = df['last_received'].str.replace('\\n\\t', ';').str.split(r';').str[-1]\n",
    "\n",
    "        if 'first_received_date' in df:\n",
    "            df['first_received_date'] = df['first_received'].str.replace('\\n\\t', ';').str.split(r';').str[-1]\n",
    "\n",
    "        def get_day_week(row):\n",
    "          date_val = row['date']\n",
    "\n",
    "          d1 = email.utils.parsedate_tz(date_val)\n",
    "\n",
    "          if d1 is None:\n",
    "            return 'NA'\n",
    "\n",
    "          try:\n",
    "            val1 = email.utils.mktime_tz(d1)\n",
    "            day = datetime.fromtimestamp(val1).strftime(\"%A\")\n",
    "          except:\n",
    "            return 'NA'\n",
    "\n",
    "          return day\n",
    "\n",
    "        df['day_of_week'] = df.apply(get_day_week, axis=1)\n",
    "\n",
    "        df['day_of_week'] = df['day_of_week'].apply(lambda x: ['NA', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'].index(x))\n",
    "\n",
    "        final_features_list.append('day_of_week')\n",
    "\n",
    "        test1 = email.utils.parsedate_tz('Mon, 12 Jul 2021 19:28:19 +0000')\n",
    "        test2 = email.utils.parsedate_tz('Mon, 12 Jul 2021 19:39:12 +0000')\n",
    "\n",
    "        print(((email.utils.mktime_tz(test2)) - (email.utils.mktime_tz(test1))) / (60*60))\n",
    "        print(((email.utils.mktime_tz(test2)) - (email.utils.mktime_tz(test1))) )\n",
    "\n",
    "        def span_time_finder(row):\n",
    "\n",
    "          if 'first_received_date' in row:\n",
    "              first = row['first_received_date']\n",
    "              d1 = email.utils.parsedate_tz(first)\n",
    "\n",
    "\n",
    "\n",
    "          if 'last_received_date' in row:\n",
    "\n",
    "              last = row['last_received_date']\n",
    "              d2 = email.utils.parsedate_tz(last)\n",
    "\n",
    "        #   if d1 is None or d2 is None:\n",
    "        #     return -1\n",
    "\n",
    "          if 'last_received_date' not in row and 'first_received_date' not in row:\n",
    "                return -1\n",
    "\n",
    "          try:\n",
    "            val1 = email.utils.mktime_tz(d1)\n",
    "            val2 = email.utils.mktime_tz(d2)\n",
    "          except:\n",
    "            return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          return (email.utils.mktime_tz(d2)) - (email.utils.mktime_tz(d1))\n",
    "\n",
    "\n",
    "\n",
    "        df['span_time'] = df.apply(span_time_finder, axis=1)\n",
    "\n",
    "        df['span_time'] = df['span_time'].apply(lambda x: 0 if x < 0 else 1 if x < 10 else 2 if x < 47 else 3 if x < 1100 else 4)\n",
    "\n",
    "        # Source: https://github.com/Te-k/pyreceived/blob/master/pyreceived/parser.py\n",
    "\n",
    "        import re\n",
    "        class ReceivedParser(object):\n",
    "            regexes = [\n",
    "                (\"from\\s+(mail\\s+pickup\\s+service|(?P[\\[\\]\\w\\.\\-]*))\\s*(\\(\\s*\\[?(?P[a-f\\d\\.\\:]+)(\\%\\d+|)\\]?\\s*\\)|)\\s*by\\s*(?P[\\w\\.\\-]+)\\s*(\\(\\s*\\[?(?P[\\d\\.\\:a-f]+)(\\%\\d+|)\\]?\\)|)\\s*(over\\s+TLS\\s+secured\\s+channel|)\\s*with\\s*(mapi|Microsoft\\s+SMTP\\s+Server|Microsoft\\s+SMTPSVC(\\((?P[\\d\\.]+)\\)|))\\s*(\\((TLS|version=(?P[\\w\\.]+)|)\\,?\\s*(cipher=(?P[\\w\\_]+)|)\\)|)\\s*(id\\s+(?P[\\d\\.]+)|)\", \"MS SMTP Server\"), #exchange\n",
    "                (\"(from\\s+(?P[\\[\\S\\]]+)\\s+\\(((?P[\\S]*)|)\\s*\\[(IPv6\\:(?P[a-f\\d\\:]+)\\:|)((?P[\\d\\.\\:]+)|)\\]\\s*(\\(may\\s+be\\s+forged\\)|)\\)\\s*(\\(using\\s+(?P[\\w\\.]+)\\s+with\\s+cipher\\s+(?P[\\w\\-]+)\\s+\\([\\w\\/\\s]+\\)\\)\\s+(\\(No\\s+client\\s+certificate\\s+requested\\)|)|)|)\\s*(\\(Authenticated\\s+sender\\:\\s+(?P[\\w\\.\\-\\@]+)\\)|)\\s*by\\s+(?P[\\S]+)\\s*(\\((?P[\\S]*)\\s*\\[((?P[a-f\\:\\d]+)|)(?P[\\d\\.]+)\\]\\)|)\\s*(\\([^\\)]*\\)|)\\s*(\\(Postfix\\)|)\\s*(with\\s+(?P\\w*)|)\\s*id\\s+(?P[\\w\\-]+)\\s*(for\\s+\\<(?P[\\w\\.\\@]+)\\>|)\", \"postfix\"), #postfix\n",
    "                (\"(from\\s+(?P[\\[\\S\\]]+)\\s+\\(((?P[\\S]*)|)\\s*\\[(IPv6\\:(?P[a-f\\d\\:]+)|)\\]\\)\\s*(\\(using\\s+(?P[\\w\\.]+)\\s+with\\s+cipher\\s+(?P[\\w\\-]+)\\s+\\([\\w\\/\\s]+\\)\\)\\s+(\\(No\\s+client\\s+certificate\\s+requested\\)|)|)|)\\s*(\\(Authenticated\\s+sender\\:\\s+(?P[\\w\\.\\-\\@]+)\\)|)\\s*by\\s+(?P[\\S]+)\\s*(\\((?P[\\S]*)\\s*\\[((?P[a-f\\:\\d]+)|)(?P[\\d\\.]+)\\]\\)|)\\s*(\\([^\\)]*\\)|)\\s*(\\(Postfix\\)|)\\s*(with\\s+(?P\\w+)|)\\s*id\\s+(?P[\\w\\-]+)\\s*(for\\s+\\<(?P[\\w\\.\\@]+)\\>|)\", \"postfix\"),#POSTFIX\n",
    "                (\"\\s*from\\s+\\[?(?P[\\d\\.\\:]+)\\]?\\s*(\\((port=\\d+|)\\s*helo=(?P[\\[\\]\\w\\.\\:\\-]+)\\)|)\\s+by\\s+(?P[\\w\\-\\.]+)\\s+with\\s+(?P\\w+)\\s*(\\((?P[\\w\\.\\:\\_\\-]+)\\)|)\\s*(\\(Exim\\s+(?P[\\d\\.\\_]+)\\)|)\\s*\\(envelope-from\\s+(?P[\\w\\@\\-\\.]*)>?\\s*\\)\\s*id\\s+(?P[\\w\\-]+)\\s*\\s*(for\\s+(?P[\\w\\.\\@]+)>?|)\", \"exim\"), #exim\n",
    "                (\"\\s*from\\s+(?P[\\w\\.]+)\\s+\\(\\[?(?P[\\d\\.\\:a-f]+)\\]?(\\:\\d+|)\\s*(helo\\=\\[?(?P[\\w\\.\\:\\-]+)|)\\]?\\)\\s+by\\s+(?P[\\w\\-\\.]+)\\s+with\\s+(?P\\w+)\\s+(\\((?P[\\w\\.\\:\\_]+)\\)|)\\s*\\(Exim\\s+(?P[\\d\\.\\_]+)\\)\\s*\\(envelope-from\\s+\\<(?P[\\w\\@\\-\\.]+)\\>\\s*\\)\\s*id\\s+(?P[\\w\\-]+)\\s*(for\\s+(?P[\\w\\.\\@]+)|)\", \"exim\"),# exim\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P\\w+)\\s+\\(Exim\\s+(?P[\\d\\.]+)\\)\\s+\\(envelope-from\\s+<*(?P[\\w\\.\\-\\@]+)>*\\)\\s+id\\s+(?P[\\w\\.\\-]+)\\s+for\\s+(?P[\\w\\.\\-\\@]+)>?\", \"exim\"), #exim\n",
    "                (\"from\\s+(?P[\\[\\]\\w\\-\\.]+)\\s+\\(((?P[\\w\\.\\-]+)|)\\s*\\[(?P[\\da-f\\.\\:]+)\\]\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(Oracle\\s+Communications\\s+Messaging\\s+Server\\s+(?P[\\w\\.\\-]+)(\\([\\d\\.]+\\)|)\\s+(32bit|64bit|)\\s*(\\([^\\)]+\\)|)\\)\\s*with\\s+(?P\\w+)\\s+id\\s+\\(?P[\\w\\@\\.\\-]+)\\>?\", \"Oracle Communication Messaging Server\"), #Oracle\n",
    "                (\"from\\s+(?P[\\w\\-\\.]+)\\s+\\(\\[(?P[\\d\\.\\:a-f]+)\\]\\s+helo=(?P[\\w\\.\\-]+)\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P\\w+)\\s+\\(ASSP\\s+(?P[\\d\\.]+)\\s*\\)\", \"ASSP\"), #ASSP\n",
    "                (\"from\\s+(?P[\\[\\]\\d\\w\\.\\-]+)\\s+\\(\\[\\[?(?P[\\d\\.]+)(\\:\\d+|)\\]\\s*(helo=(?P[\\w\\.\\-]+)|)\\s*\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(envelope-from\\s+\\(?P[^>]+)\\>?\\)\\s+\\(ecelerity\\s+(?P[\\d\\.]+)\\s+r\\([\\w\\-\\:\\.]+\\)\\)\\s+with\\s+(?P\\w+)\\s*(\\(cipher=(?P[\\w\\-\\_]+)\\)|)\\s*id\\s+(?P[\\.\\-\\w\\/]+)\", \"ecelerity\"), #ecelerity\n",
    "                (\"from\\s+(?P[\\[\\]\\w\\.\\-]+)\\s+\\(((?P[\\w\\.\\-]+)|)\\s*(\\[(?P[\\d\\.\\:a-f]+)\\]|)\\)\\s*by\\s+(?P[\\w\\.\\-]+)\\s+(\\([\\w\\.\\-\\=]+\\)|)\\s+with\\s+(?P\\w+)\\s+\\(Nemesis\\)\\s+id\\s+(?P[\\w\\.\\-]+)\\s*(for\\s+\\(?P[\\w\\.\\@\\-]+)\\>?|)\", \"nemesis\"), #nemesis\n",
    "                (\"\\(qmail\\s+\\d+\\s+invoked\\s+(from\\s+network|)(by\\s+uid\\s+\\d+|)\\)\", \"qmail\"), #WTF qmail\n",
    "                (\"from\\s+\\[?(?P[\\d\\.a-f\\:]+)\\]?\\s+\\(account\\s+(?P[\\w\\.\\@\\-]+)>?\\s+HELO\\s+(?P[\\w\\.\\-]+)\\)\\s+by\\s+(?P[\\w\\.\\-]*)\\s+\\(CommuniGate\\s+Pro\\s+SMTP\\s+(?P[\\d\\.]+)\\)\\s+with\\s+(?P\\w+)\\s+id\\s+(?P[\\w\\-\\.]+)\\s+for\\s+(?P[\\w\\.\\-\\@]+)>?\", \"CommuniGate\"), #CommuniGate\n",
    "                (\"from\\s+(?P[\\d\\.\\:a-f]+)\\s+\\(SquirrelMail\\s+authenticated\\s+user\\s+(?P[\\w\\@\\.\\-]+)\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P\\w+)\", \"SquirrelMail\"),\n",
    "                (\"by\\s+(?P[\\w\\.\\-]+)\\s+\\((?P\\w+)\\s+sendmail\\s*(emulation|)\\)\", \"sendmail\"), #sendmail\n",
    "                (\"from\\s+(?P[\\[\\]\\w\\.\\-]+)\\s+\\(\\[(?P[\\w\\.\\-]+)\\]\\s+\\[(?P[\\d\\.a-f\\:]+)\\]\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(Sun\\s+Java\\(tm\\)\\s+System\\s+Messaging\\s+Server\\s+(?P[\\w\\.\\-]+)\\s+\\d+bit\\s+\\(built\\s+\\w+\\s+\\d+\\s+\\d+\\)\\)\\s+with\\s+(?P\\w+)\\s+id\\s+(?P[\\w\\.\\-\\@]+)>?\", \"Sun Java System Messaging Server\"), # Sun Java System Messaging Server\n",
    "                (\"from\\s+(?P[\\w\\.\\-\\[\\]]+)\\s+\\((?P[\\d\\.a-f\\:]+)\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(Axigen\\)\\s+with\\s+(?P\\w+)\\s+id\\s+(?P[\\w\\.\\-]+)\", \"Axigen\"), #axigen\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+\\((?P[\\w\\.\\-]+)\\s+\\[(?P[\\d\\.a-f\\:]+)\\]\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(Horde\\s+MIME\\s+library\\)\\s+with\\s+(?P\\w+)\", \"Horde MIME library\"), #Horde\n",
    "                (\"from\\s+(?P[\\w\\.\\-\\[\\]]+)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(PGP\\s+Universal\\s+Service\\)\", \"PGP Universal Service\", \"local\"), # PGP Universal Service\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P\\w+)\\s+\\(Sophos\\s+PureMessage\\s+Version\\s+(?P[\\d\\.\\-]+)\\)\\s+id\\s+(?P[\\w\\.\\-]+)\\s+for\\s+(?P[\\w\\.\\-\\@]+)\", \"Sophos PureMessage\"), #Sophos PureMessage\n",
    "                (\"by\\s+(?P[\\d\\.\\:a-f]+)\\s+with\\s+(?P\\w+)\", \"unknown\"), # other\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+\\#?\\s*(\\(|\\[|\\(\\[)\\s*(?P[\\d\\.\\:a-f]+)\\s*(\\]|\\)|\\]\\))\\s+by\\s+(?P[\\w\\.\\-]+)(\\s+\\([\\w\\.\\s\\/]+\\)|)\\s*(with\\s+(?P\\w+)|)\\s*(id\\s+(?P[\\w]+)|)(\\(\\-\\)|)\\s*(for\\s+\\<(?P[\\w\\@\\.]+)\\>?|)\", \"unknown\"), #unknown\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s*\\(HELO\\s+(?P[\\w\\.\\-]+)\\)\\s*\\(\\[?(?P[\\d\\.\\:a-f]+)\\]?\\)\\s+by\\s+(?P[\\w\\.\\-]+)(\\s+\\([\\d\\.]+\\)|)\\s*(with\\s+(?P\\w+)|)\\s*(id\\s+(?P[\\w]+)|)(\\(\\-\\)|)\", \"unknown\"), #other other\n",
    "                (\"from\\s+([\\(\\[](?P[\\d\\.\\:a-f]+)[\\)\\]]|)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+id\\s+(?P\\w+)\\s*(with\\s+(?P\\w+)|)\\s*\\s*(for\\s+\\<(?P[\\w\\@\\.\\-]+)\\>|)\", \"unknown\"),#other\n",
    "                (\"from\\s+(?P[\\w\\.]+)\\s+(\\(HELO\\s+(?P[\\w\\.\\-]+)\\)|)\\s*(\\((?P[\\da-f\\.\\:]+)\\)|)\\s*by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P[\\w\\-]+)\\s+encrypted\\s+SMTP\", \"unknown\"), #unknown\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+(\\(HELO\\s+(?P[\\w\\.\\-]+)\\)|)\\s+\\((?P[\\w\\.]+\\@[\\w\\.]+)\\@(?P[\\da-d\\.\\:]+)\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+with\\s+(?P\\w+)\", \"unknown\"), #unknown\n",
    "                (\"from\\s+(?P[\\w\\.\\-]+)\\s+\\(HELO\\s+(?P[\\w\\.\\-\\?]+)\\)\\s+\\(\\w+\\@[\\w\\.]+\\@(?P[\\d\\.a-f\\-]+)_\\w+\\)\\s+by\\s+(?P[\\w\\.\\-\\:]+)\\s+with\\s+(?P\\w+)\", \"unknown\"), #unknown\n",
    "                (\"from\\s+(?P[\\w\\.\\-\\[\\]]+)\\s+\\(\\[(?P[\\da-f\\.\\:]+)\\]\\)\\s+by\\s+(?P[\\w\\.\\-]+)\\s+\\(\\[(?P[\\d\\.a-f\\:]+)\\]\\)\\s+with\\s+(?P\\w+)\", \"unknown\"), #unknown\n",
    "                ]\n",
    "            @staticmethod\n",
    "            def parse(header):\n",
    "                parts = header.split(\";\")\n",
    "                if len(parts) != 2:\n",
    "                    return None\n",
    "\n",
    "                data = {}\n",
    "\n",
    "                # parse the hard part\n",
    "                found = False\n",
    "                for regex in ReceivedParser.regexes:\n",
    "                    match = re.match(regex[0], parts[0], re.IGNORECASE)\n",
    "                    if match:\n",
    "                        data['server'] = regex[1]\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "                if not found:\n",
    "                    return None\n",
    "                return {**data, **match.groupdict()}\n",
    "\n",
    "\n",
    "\n",
    "        received_parser = ReceivedParser()\n",
    "\n",
    "        def check_if_valid(dict_to_check, str_val):\n",
    "          if dict_to_check is None:\n",
    "            return False\n",
    "          elif str_val not in dict_to_check:\n",
    "            return False\n",
    "          elif dict_to_check[str_val] is None:\n",
    "            return False\n",
    "          else:\n",
    "            return True\n",
    "\n",
    "        # One-Hot encoding, new columns: conseq_num_received_is_one, conseq_received_good, conseq_received_bad, conseq_received_unknown (All values are 0 or 1)\n",
    "\n",
    "        def check_conseq_received_domain(row):\n",
    "          num_received = row['hops']\n",
    "          if num_received == 1:\n",
    "            return pd.Series([1,0,0,0])\n",
    "\n",
    "          col_name_base = 'received'\n",
    "          for i in range(1, num_received):\n",
    "            curr_col = col_name_base + str(i)\n",
    "            next_col = col_name_base + str(i+1)\n",
    "\n",
    "            curr_val = row[curr_col]\n",
    "            next_val = row[next_col]\n",
    "\n",
    "            from_vals_dict = received_parser.parse(curr_val)\n",
    "            by_val_dict = received_parser.parse(next_val)\n",
    "\n",
    "            by_valid = check_if_valid(by_val_dict, 'by_hostname')\n",
    "            from_hostname_valid = check_if_valid(from_vals_dict, 'from_hostname')\n",
    "            from_name_valid = check_if_valid(from_vals_dict, 'from_name')\n",
    "\n",
    "            if by_valid:\n",
    "              # Both are valid case\n",
    "              if from_hostname_valid and from_name_valid:\n",
    "                if from_vals_dict['from_name'] == by_val_dict['by_hostname'] or \\\n",
    "                from_vals_dict['from_hostname'] == by_val_dict['by_hostname']:\n",
    "                  continue\n",
    "                else:\n",
    "                  #print(\"K Val: \" + str(i) +\"  FROMHOSTNAME: \" + from_vals_dict['from_hostname'] + \"  FROMNAME: \" + from_vals_dict['from_name'] + \"   BY: \" + by_val_dict['by_hostname'])\n",
    "                  return pd.Series([0,0,1,0])\n",
    "\n",
    "              # Neither are valid case\n",
    "              elif not from_hostname_valid and not from_name_valid:\n",
    "                # Check my way before saying its invalid\n",
    "                val = my_checks(curr_val, next_val)\n",
    "                if isinstance(val, str):\n",
    "                  continue\n",
    "                else:\n",
    "                  return val\n",
    "\n",
    "              # From hostname is valid case\n",
    "              elif from_hostname_valid:\n",
    "                if from_vals_dict['from_hostname'] == by_val_dict['by_hostname']:\n",
    "                  continue\n",
    "                else:\n",
    "                  #print(\"K Val: \" + str(i) +\"  FROMHOSTNAME: \" + from_vals_dict['from_hostname'] + \"   BY: \" + by_val_dict['by_hostname'])\n",
    "                  return pd.Series([0,0,1,0])\n",
    "\n",
    "              # From name is valid case\n",
    "              elif from_name_valid:\n",
    "                if from_vals_dict['from_name'] == by_val_dict['by_hostname']:\n",
    "                  continue\n",
    "                else:\n",
    "                  #print(\"K Val: \" + str(i) + \"  FROMNAME: \" + from_vals_dict['from_name'] + \"   BY: \" + by_val_dict['by_hostname'])\n",
    "                  return pd.Series([0,0,1,0])\n",
    "            else:\n",
    "              # Check my way before saying its invalid\n",
    "              val = my_checks(curr_val, next_val)\n",
    "              if isinstance(val, str):\n",
    "                continue\n",
    "              else:\n",
    "                return val\n",
    "\n",
    "          # All checks worked out, return a good result\n",
    "          return pd.Series([0,1,0,0])\n",
    "\n",
    "\n",
    "        def my_checks(curr_val, next_val):\n",
    "          first_domain_from = re.search(r'((?<=\\bfrom\\s)[^\\s]+)', curr_val)\n",
    "          second_domain_from = re.search(r'((?<=\\().*?(?=\\[))', curr_val)\n",
    "          domain_by = re.search(r'((?<=\\bby\\s)[^\\s]+)', next_val)\n",
    "\n",
    "          if domain_by is not None:\n",
    "            # Both are valid case\n",
    "            if first_domain_from is not None and second_domain_from is not None:\n",
    "                if first_domain_from.group(0) == domain_by.group(0) or \\\n",
    "                second_domain_from.group(0) == domain_by.group(0):\n",
    "                  return 'continue'\n",
    "                else:\n",
    "                  return pd.Series([0,0,1,0])\n",
    "\n",
    "            # Neither are valid case\n",
    "            if first_domain_from is None and second_domain_from is None:\n",
    "              return pd.Series([0,0,0,1]) \n",
    "\n",
    "            # One is valid case\n",
    "            if first_domain_from is not None:\n",
    "              if first_domain_from.group(0) == domain_by.group(0):\n",
    "                return 'continue'\n",
    "              else:\n",
    "                return pd.Series([0,0,1,0])\n",
    "\n",
    "            # One is valid case\n",
    "            if second_domain_from is not None:\n",
    "              if second_domain_from.group(0) == domain_by.group(0):\n",
    "                return 'continue'\n",
    "              else:\n",
    "                return pd.Series([0,0,1,0])\n",
    "\n",
    "          else:\n",
    "            return pd.Series([0,0,0,1])\n",
    "\n",
    "\n",
    "        from difflib import SequenceMatcher\n",
    "\n",
    "        def simScore(a, b):\n",
    "          if isinstance(a, float) or isinstance(b, float):\n",
    "            return 0\n",
    "          return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "        # emails in brackets '<>' are matched first, and if none, then other emails are matched\n",
    "        def extract_emails(row, col_name):\n",
    "\n",
    "          in_brackets = re.findall(r'<([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)>', row[col_name])\n",
    "\n",
    "          if len(in_brackets) == 0:\n",
    "            not_in_brackets = re.findall(r'([a-zA-Z0-9+._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)', row[col_name])\n",
    "            if len(not_in_brackets) == 0:\n",
    "              return []\n",
    "            else:\n",
    "              return not_in_brackets\n",
    "          else:\n",
    "            return in_brackets\n",
    "\n",
    "\n",
    "\n",
    "        newflag = 0\n",
    "        newflag1 = 0\n",
    "        newflag2 = 0\n",
    "        newflag3 = 0\n",
    "        newflag4 =0 \n",
    "        newflag5 = 0\n",
    "        newflag6 = 0\n",
    "        newflag7 = 0\n",
    "        newflag8 = 0\n",
    "        newflag9 = 0\n",
    "\n",
    "        if 'from' in df:\n",
    "            emails_from = df.apply(extract_emails, col_name='from', axis=1)\n",
    "            newflag1 = 1\n",
    "\n",
    "        if 'message-id' in df:\n",
    "            emails_message_id = df.apply(extract_emails, col_name='message-id', axis=1)\n",
    "            newflag2 = 1\n",
    "\n",
    "        if 'return-path' in df:\n",
    "            emails_return_path = df.apply(extract_emails, col_name='return-path', axis=1)\n",
    "            newflag3 = 1\n",
    "\n",
    "        if 'reply-to' in df:\n",
    "            emails_reply_to = df.apply(extract_emails, col_name='reply-to', axis=1)\n",
    "            newflag4 = 1\n",
    "\n",
    "        if 'errors-to' in df:\n",
    "            emails_errors_to = df.apply(extract_emails, col_name='errors-to', axis=1)\n",
    "            newflag = 1\n",
    "\n",
    "        if 'in-reply-to' in df:\n",
    "            emails_in_reply_to = df.apply(extract_emails, col_name='in-reply-to', axis=1)\n",
    "            newflag5 = 1\n",
    "\n",
    "        if 'references' in df:\n",
    "            emails_references = df.apply(extract_emails, col_name='references', axis=1)\n",
    "            newflag6 = 1\n",
    "\n",
    "        if 'to' in df:\n",
    "            emails_to = df.apply(extract_emails, col_name='to', axis=1)\n",
    "            newflag6 = 1\n",
    "\n",
    "        if 'cc' in df:\n",
    "            emails_cc = df.apply(extract_emails, col_name='cc', axis=1)\n",
    "            newflag7 = 1\n",
    "\n",
    "        if 'sender' in df:\n",
    "            emails_sender = df.apply(extract_emails, col_name='sender', axis=1)\n",
    "            newflag8 = 1\n",
    "\n",
    "        #simScores = domains_df[['return', 'from']].apply(lambda x: simScore(*x), axis=1)\n",
    "        #df['SimScore_return_from'] = simScores\n",
    "\n",
    "\n",
    "        if newflag1==1:\n",
    "            if newflag2 == 1:\n",
    "\n",
    "                if newflag3 == 1:\n",
    "\n",
    "                    if newflag4==1:\n",
    "\n",
    "                        if newflag5==1:\n",
    "\n",
    "                            if newflag6 == 1:\n",
    "\n",
    "\n",
    "                                if newflag7==1:\n",
    "\n",
    "                                    if newflag8==1:\n",
    "\n",
    "                                        if newflag == 1:\n",
    "\n",
    "                                            emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                                    emails_errors_to, emails_reply_to, emails_in_reply_to, \n",
    "                                                                    emails_references, emails_to, emails_cc, emails_sender], axis=1)\n",
    "                                            emails_df.set_axis(['from', 'message-id', 'return-path', 'errors-to', 'reply-to',\n",
    "                                                                 'in-reply-to', 'references', 'to', 'cc', 'sender'], \n",
    "                                                                axis=1, inplace=True)\n",
    "                                        else:\n",
    "                                            emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                                     emails_reply_to, emails_in_reply_to, \n",
    "                                                                    emails_references, emails_to, emails_cc, emails_sender], axis=1)\n",
    "                                            emails_df.set_axis(['from', 'message-id', 'return-path', 'reply-to',\n",
    "                                                                 'in-reply-to', 'references', 'to', 'cc', 'sender'], \n",
    "                                                                axis=1, inplace=True)\n",
    "\n",
    "                                    else:\n",
    "                                        emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                                emails_errors_to, emails_reply_to, emails_in_reply_to, \n",
    "                                                                emails_references, emails_to, emails_cc], axis=1)\n",
    "                                        emails_df.set_axis(['from', 'message-id', 'return-path', 'reply-to',\n",
    "                                                             'in-reply-to', 'references', 'to', 'cc'], \n",
    "                                                            axis=1, inplace=True)\n",
    "                                else:\n",
    "                                    emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                             emails_reply_to, emails_in_reply_to, \n",
    "                                                            emails_references, emails_to], axis=1)\n",
    "                                    emails_df.set_axis(['from', 'message-id', 'return-path', 'reply-to',\n",
    "                                                         'in-reply-to', 'references', 'to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "\n",
    "                            else:\n",
    "                                emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                             emails_reply_to, emails_in_reply_to, \n",
    "                                                             emails_to], axis=1)\n",
    "                                emails_df.set_axis(['from', 'message-id', 'return-path', 'reply-to',\n",
    "                                                         'in-reply-to', 'to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "\n",
    "                        else:\n",
    "                            emails_df = pd.concat([emails_from, emails_message_id, emails_return_path, \n",
    "                                                           emails_reply_to, \n",
    "                                                             emails_to], axis=1)\n",
    "                            emails_df.set_axis(['from', 'message-id', 'return-path', 'reply-to',\n",
    "                                                          'to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "\n",
    "                    else:\n",
    "                        emails_df = pd.concat([emails_from, emails_message_id, emails_return_path,\n",
    "\n",
    "                                                             emails_to], axis=1)\n",
    "                        emails_df.set_axis(['from', 'message-id','return-path',\n",
    "                                                          'to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "\n",
    "                else:\n",
    "                    emails_df = pd.concat([emails_from,emails_message_id, emails_to], axis=1)\n",
    "                    emails_df.set_axis(['from', 'message-id','to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "            else:\n",
    "                emails_df = pd.concat([emails_from, emails_to], axis=1)\n",
    "                emails_df.set_axis(['from', 'to'], \n",
    "                                                        axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        else:\n",
    "            emails_df = pd.concat([], axis=1)\n",
    "            emails_df.set_axis([], \n",
    "                                axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        parser = ReceivedParser()\n",
    "        test = parser.parse('from cnnimail22.cnn.com (cnnimail22.cnn.com [64.236.25.79])\\n\\tby speedy.uwaterloo.ca (8.12.8/8.12.5) with ESMTP id l39IYQ0I018124\\n\\tfor  Mon, 9 Apr 2007 14:34:26 -0400')\n",
    "\n",
    "        def get_for_domain_last_received(row):\n",
    "          last_received_val = row['last_received']\n",
    "          parsed_val = parser.parse(last_received_val)\n",
    "\n",
    "          if check_if_valid(parsed_val, 'envelope_for'):\n",
    "            main_domain = parsed_val['envelope_for'].split('@')[-1]\n",
    "            main_domain = main_domain.split('.')[-2:]\n",
    "            main_domain = main_domain[0] + '.' + re.sub('\\W+','', main_domain[1])\n",
    "            return main_domain.lower()\n",
    "          else:\n",
    "            return 'NA'\n",
    "\n",
    "        def check_for_received_domain_equal(row, field_name):\n",
    "          if field_name in row:\n",
    "              field_vals = row[field_name]\n",
    "\n",
    "              for item in field_vals:\n",
    "                if item == get_for_domain_last_received(row):\n",
    "                  return 1\n",
    "          return 0\n",
    "\n",
    "        df['domain_match_to_received'] = df.apply(check_for_received_domain_equal, field_name='to_domains', axis=1)\n",
    "        df['domain_match_to_received'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "        for item in final_features_list:\n",
    "\n",
    "          if item in df:  \n",
    "              print(item)\n",
    "\n",
    "              print(df[item].value_counts())\n",
    "              print('\\n')\n",
    "\n",
    "\n",
    "        df[df['str_to_chevron'] == 0]['label'].value_counts()\n",
    "\n",
    "\n",
    "        # Removes missing_received fields since received1 is always there, and the other ones\n",
    "        # encode the same information as 'hops'. The other removed features have only one value,\n",
    "        # or a strong majority towards one value.\n",
    "        remove_list = ['missing_received1', 'missing_received2', 'missing_received3',\n",
    "         'missing_received4', 'missing_received5', 'missing_received6',\n",
    "         'missing_received7', 'missing_received8', 'missing_received9',\n",
    "         'missing_received10', 'missing_received11', 'missing_received12',\n",
    "         'missing_received13', 'missing_received14', 'missing_received15',\n",
    "         'missing_received16', 'missing_date', 'missing_message-id', 'missing_from',\n",
    "         'missing_return-path', 'str_to_undisclosed', 'str_return-path_empty',\n",
    "         'str_from_exclam', 'str_reply-to_question', 'str_received-SPF_bad', \n",
    "         'str_received-SPF_softfail', 'str_received-SPF_fail', 'str_reply-to_question', \n",
    "         'conseq_received_unknown', 'num_recipients_from', \n",
    "         'domain_match_reply-to_received', 'domain_match_return-path_received',\n",
    "         'domain_match_from_received']\n",
    "\n",
    "        for v in remove_list:\n",
    "          if v in final_features_list:\n",
    "            final_features_list.remove(v)\n",
    "\n",
    "        final_features_list.append('label')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # final_fet = final_features_list\n",
    "\n",
    "        # for item in final_fet:\n",
    "        #     if item not in df:\n",
    "        #         final_fet.remove(item)\n",
    "\n",
    "        # df_final = df[final_fet]\n",
    "\n",
    "        for item in final_features_list:\n",
    "            if item not in df:\n",
    "                df[item] = 0\n",
    "\n",
    "\n",
    "\n",
    "        df_final = df[final_features_list]\n",
    "\n",
    "\n",
    "        rand_list = [0,1]\n",
    "\n",
    "        for item in final_features_list:\n",
    "            if item not in df:\n",
    "                df[item] = int(random.choices(rand_list, weights=(55,45), k=2))\n",
    "\n",
    "\n",
    "\n",
    "        df_final = df[final_features_list]\n",
    "\n",
    "        df_final.to_csv('preprocessed_spam_ham_new.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        inp_df = pd.read_csv(\"preprocessed_spam_ham_new.csv\")\n",
    "\n",
    "        df_row = inp_df.iloc[0:1]\n",
    "\n",
    "        df_row = df_row.iloc[: , :45]\n",
    "\n",
    "\n",
    "        pred = clf.predict(df_row)\n",
    "    \n",
    "    except:\n",
    "        print(\"Ignored\")\n",
    "        return -1\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b2289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f36f37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vizer =  TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c037be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContentDetectionTrain(content):\n",
    "\n",
    "    df = pd.read_csv(\"spam.csv\")\n",
    "\n",
    "    df.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    df.groupby('Category').describe()\n",
    "    \n",
    "    df['spam']=df['Category'].apply(lambda x: 1 if x=='spam' else 0)\n",
    "\n",
    "    df.head()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(df.Message,df.spam,test_size=0.20)\n",
    "\n",
    "\n",
    "    X_train.describe()\n",
    "    \n",
    "    X_test.describe()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    X_train_count= vizer.fit_transform(X_train.values)\n",
    "\n",
    "\n",
    "    print(X_train_count.toarray())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "\n",
    "    model.fit(X_train_count,Y_train)\n",
    "    emails = []\n",
    "    emails.append(content)\n",
    "    \n",
    "    emails_count = vizer.transform(emails)\n",
    "    \n",
    "    res=model.predict(emails_count)\n",
    "\n",
    "    return res[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f9609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122076f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d345c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7fa53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54dfe995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageDetectionInput(img_path,modl='./model.pkl'):\n",
    "    model = pickle.load(open(modl,'rb'))\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((200,200))\n",
    "        imr = np.array(img)/255\n",
    "        imr = imr.reshape([200,200,3])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error\")\n",
    "        return -1\n",
    "    \n",
    "    imr = imr.reshape(1,200,200,3)\n",
    "    prediction = model.prediction(imr)\n",
    "    return 0 if prediction < 0.5 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46694e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65a17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b0a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca97d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b4913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184ef19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d76b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.68708565367277\n",
      "F1 Score: 99.76531112849547\n",
      "Recall: 99.87787718692755\n",
      "Precision: 99.65299851663488\n",
      "ROC AUC: 99.5923410699163\n",
      "Confusion Matrix: [[18767   131]\n",
      " [   46 37621]]\n",
      "-----------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n",
      "c:\\Users\\adhi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.69415716432422\n",
      "F1 Score: 99.77044438252193\n",
      "Recall: 99.808851249104\n",
      "Precision: 99.73206706281834\n",
      "ROC AUC: 99.63720157967953\n",
      "Confusion Matrix: [[18797   101]\n",
      " [   72 37595]]\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "HeaderTrain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4126c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09063d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb326b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff5d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headerfile = \"headertext.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b1f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"Wow. I never realized that you were so embarrassed by your accommodations. I thought you liked it, since I was doing the best I could and you always seemed so happy about \\\"the cave\\\". I'm sorry I didn't and don't have more to give. I'm sorry I offered. I'm sorry your room was so embarrassing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4832efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e27f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'zzz_0969_d70494b85b_m.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acff47e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "pred1 = HeaderInput(headerfile)\n",
    "pred2 = ContentDetectionTrain(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce790b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred3 = ImageDetectionInput(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa74da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mail is Ham\n",
      "The mail is Ham\n"
     ]
    }
   ],
   "source": [
    "if pred1==-1:\n",
    "    if pred2 == 1:\n",
    "        print(\"The mail is Spam\")\n",
    "    else:\n",
    "        print(\"The mail is Ham\")\n",
    "        \n",
    "# elif pred1 == 1:\n",
    "#     if pred2 == 1:\n",
    "#         print(\"The mail is Spam\")\n",
    "#     else:\n",
    "#         print(\"The mail is Ham\")\n",
    "# else:\n",
    "#     if pred2 == 1:\n",
    "#         print(\"The mail is Spam\")\n",
    "#     else:\n",
    "#         print(\"The mail is Ham\")\n",
    "    \n",
    "\n",
    "\n",
    "if pred1 ==1 or pred2 == 1:\n",
    "    print(\"The mail is Spam\")\n",
    "else:\n",
    "    print(\"The mail is Ham\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a607dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statistics\n",
    "\n",
    "\n",
    "\n",
    "# def final_pred(p1,p2,p3):\n",
    "    \n",
    "#     final_ans = statistics.mode(p1,p2,p2)\n",
    "    \n",
    "#     return final_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95401a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f495de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prediction = final_pred(pred1,pred2,pred3)\n",
    "\n",
    "# if final_prediction == 1:\n",
    "#     print(\"The Mail is Spam\")\n",
    "    \n",
    "# else:\n",
    "#     print(\"The Mail is Ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a7eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d0a37138ffd9c2a5f5132427dd3a113582efc397067d69de3686026f03bbd13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
